{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyathena import connect\n",
    "import pandas as pd\n",
    "import configparser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-328bd8bf4bf5>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-328bd8bf4bf5>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    aws configure\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "aws configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('../access_keys.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyathena import connect\n",
    "import pandas as pd\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('../access_keys.cfg')\n",
    "AWS_ACCESS_KEY = config.get('aws', 'aws_access_key')\n",
    "AWS_SECRET_KEY = config.get('aws', 'aws_secret_key')\n",
    "from six.moves.urllib.parse import quote_plus\n",
    "from sqlalchemy.engine import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA_NAME = \"yelp\"\n",
    "S3_STAGING_DIR = \"s3://sairin.yelp.dataset/dataset\"\n",
    "AWS_REGION = \"us-west-1\"\n",
    "conn_str = (\n",
    "    \"awsathena+rest://{aws_access_key_id}:{aws_secret_access_key}@\"\n",
    "    \"athena.{region_name}.amazonaws.com:443/\"\n",
    "    \"{schema_name}?s3_staging_dir={s3_staging_dir}&work_group=primary\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\n",
    "    conn_str.format(\n",
    "        aws_access_key_id=quote_plus(AWS_ACCESS_KEY),\n",
    "        aws_secret_access_key=quote_plus(AWS_SECRET_KEY),\n",
    "        region_name=AWS_REGION,\n",
    "        schema_name=SCHEMA_NAME,\n",
    "        s3_staging_dir=quote_plus(S3_STAGING_DIR),\n",
    "    )\n",
    ")\n",
    "athena_connection = engine.connect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AWS_ACCESS_KEY' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-59df6b62a401>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m engine = create_engine(\n\u001b[1;32m     17\u001b[0m     conn_str.format(\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0maws_access_key_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquote_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAWS_ACCESS_KEY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0maws_secret_access_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquote_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAWS_SECRET_KEY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mregion_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAWS_REGION\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AWS_ACCESS_KEY' is not defined"
     ]
    }
   ],
   "source": [
    "from six.moves.urllib.parse import quote_plus\n",
    "from sqlalchemy.engine import create_engine\n",
    "#AWS_ACCESS_KEY = \"AKIA5644JC6274ZTUJXC\"\n",
    "#AWS_SECRET_KEY = \"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\"\n",
    "SCHEMA_NAME = \"yelp\"\n",
    "S3_STAGING_DIR = \"s3://sairin.yelp.dataset/dataset\"\n",
    "AWS_REGION = \"us-west-1\"\n",
    "conn_str = (\n",
    "    \"awsathena+rest://{aws_access_key_id}:{aws_secret_access_key}@\"\n",
    "    \"athena.{region_name}.amazonaws.com:443/\"\n",
    "    \"{schema_name}?s3_staging_dir={s3_staging_dir}&work_group=primary\"\n",
    ")\n",
    "\n",
    "\n",
    "# Create the SQLAlchemy connection. Note that you need to have pyathena installed for this.\n",
    "engine = create_engine(\n",
    "    conn_str.format(\n",
    "        aws_access_key_id=quote_plus(AWS_ACCESS_KEY),\n",
    "        aws_secret_access_key=quote_plus(AWS_SECRET_KEY),\n",
    "        region_name=AWS_REGION,\n",
    "        schema_name=SCHEMA_NAME,\n",
    "        s3_staging_dir=quote_plus(S3_STAGING_DIR),\n",
    "    )\n",
    ")\n",
    "athena_connection = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of Tables used in project\n",
    "\n",
    "1.businesses\n",
    "\n",
    "2.checkins\n",
    "\n",
    "3.reviews\n",
    "\n",
    "4.tips\n",
    "\n",
    "5.users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying review table to check top 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM yelp.reviews\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "athena_connection = engine.connect()\n",
    "df = pd.read_sql(query, athena_connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"\"\"\n",
    "        SELECT * \n",
    "        FROM yelp.reviews\n",
    "        where business_id='uQJNNTRWTj1SYtapTL0y_A'\n",
    "        \"\"\"\n",
    "\n",
    "df_review_one = pd.read_sql(query1, athena_connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_business = \"\"\"\n",
    "        SELECT * \n",
    "        FROM yelp.businesses\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "\n",
    "df_business = pd.read_sql(query_business, athena_connection)\n",
    "df_business.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df_business[df_business['categories'].str.contains('Restaurant', case=False)]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_json_chunks(file_path, chunk_size):\n",
    "    def json_generator():\n",
    "        for chunk in pd.read_json(file_path, lines=True, chunksize=chunk_size):\n",
    "            yield chunk\n",
    "\n",
    "    generator = json_generator()\n",
    "    df = next(generator)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your JSON file\n",
    "review_json_file_path = '/Users/sairindhri/Documents/Umich/Milestone2/yelp-data-analysis/notebooks/yelp_dataset/yelp_academic_dataset_review.json'\n",
    "business_json_file_path = '/Users/sairindhri/Documents/Umich/Milestone2/yelp-data-analysis/notebooks/yelp_dataset/yelp_academic_dataset_business.json'\n",
    "\n",
    "# Define the chunk size\n",
    "chunk_size = 100000\n",
    "\n",
    "# Read review data\n",
    "df_review = read_json_chunks(review_json_file_path, chunk_size)\n",
    "#print(\"Review DataFrame:\")\n",
    "df_review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read business data\n",
    "df_business = read_json_chunks(business_json_file_path, chunk_size)\n",
    "#print(\"\\nBusiness DataFrame:\")\n",
    "df_business.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants = df_business[df_business['categories'].notna() & df_business['categories'].str.contains('Restaurant', case=False)]\n",
    "merged_df = df_review.merge(df_restaurants, on='business_id')\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['review_id', 'user_id', 'business_id', 'stars_x', 'useful', 'funny', 'cool', 'text', 'date', 'name', 'address', 'city', 'state', 'postal_code', 'latitude', 'longitude', 'review_count', 'is_open', 'attributes', 'categories', 'hours']\n",
    "filtered_restaurant_df = merged_df[selected_columns].rename(columns={'stars_x': 'stars'})\n",
    "filtered_restaurant_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_final=filtered_restaurant_df[['text','stars']]\n",
    "filtered_df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid running this code block, instead take the data from the pickle\n",
    "runThisBlock = False\n",
    "\n",
    "if runThisBlock:\n",
    "    # Load the pre-trained model\n",
    "    bert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    # Create a new column for embeddings\n",
    "    filtered_df_final['embeddings'] = None\n",
    "\n",
    "    def generate_mean_embedding(row):\n",
    "        review = row\n",
    "        review_sentances = nltk.sent_tokenize(review)\n",
    "\n",
    "        embeddings = bert_model.encode(review_sentances)\n",
    "\n",
    "        mean_embeddings =  np.mean(embeddings, axis=0)\n",
    "        return mean_embeddings\n",
    "\n",
    "    total_reviews = filtered_df_final.shape[0]\n",
    "    progress_bar = tqdm(total=total_reviews, desc='Processing Reviews')\n",
    "\n",
    "    for index, row in filtered_df_final.iterrows():\n",
    "        review_text = row['text']\n",
    "        mean_embeddings=generate_mean_embedding(review_text)\n",
    "        filtered_df_final.at[index, 'embeddings'] = mean_embeddings\n",
    "        # Update the progress bar\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "    # Close the progress bar\n",
    "    progress_bar.close()\n",
    "    \n",
    "    \n",
    "    filtered_df_final.head()\n",
    "\n",
    "    filtered_df_final.to_pickle(\"intermediate_data/bert_generated_embeddings_dataframe.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DataFrame from pickle file\n",
    "df_restored = pd.read_pickle(\"intermediate_data/bert_generated_embeddings_dataframe.pkl\")\n",
    "\n",
    "# Now you can use 'df' just like a regular DataFrame\n",
    "\n",
    "df_restored.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=df_restored[['text','stars','embeddings']]\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_sentiment(star_rating):\n",
    "    if star_rating in [1, 2]:\n",
    "        return '-1'\n",
    "    elif star_rating == 3:\n",
    "        return '0'\n",
    "    elif star_rating in [4, 5]:\n",
    "        return '1'\n",
    "\n",
    "# Apply the mapping function to create the 'sentiment' column\n",
    "df_final['sentiment'] = df_final['stars'].apply(map_sentiment)\n",
    "df_final.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and split the dataframe into train_df, dev_df, test_df\n",
    "train_df, dev_df, test_df = np.split(df_final.sample(frac=1, random_state=42), \n",
    "                       [int(.8*len(df_final)), int(.9*len(df_final))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists of embeddings into a 2D numpy array\n",
    "X_train = np.stack(train_df['embeddings'].values)\n",
    "\n",
    "# The 'stars' column is your target variable\n",
    "y_train = train_df['sentiment'].values\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to your data (without specifying number of components to keep all components)\n",
    "pca = PCA()\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "\n",
    "# Get explained variance ratios from PCA using all features\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# Generate Scree plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(np.cumsum(explained_variance))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Scree Plot - Cumulative Explained Variance by Number of Components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=128)\n",
    "\n",
    "X_train_reduced = pca.fit_transform(X_train)\n",
    "\n",
    "#print(\"Reduced shape:\", X_train_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.stack(dev_df['embeddings'].values)\n",
    "\n",
    "X_test_reduced = pca.transform(X_test)\n",
    "\n",
    "# The 'stars' column is your target variable\n",
    "y_test = dev_df['sentiment'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models to test\n",
    "models = [\n",
    "    {\"name\": \"Logistic Regression\", \"model\": LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)},\n",
    "    {\"name\": \"Random Forest\", \"model\": RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, max_features='auto', random_state=42)},\n",
    "     {\"name\": \"SVM\", \"model\": SVC(C=1.0, kernel='rbf', gamma='scale')}\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store the performance metrics\n",
    "metrics = []\n",
    "\n",
    "\n",
    "# Loop through the models list\n",
    "for model in models:\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    model['model'].fit(X_train_reduced, y_train)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    y_pred = model['model'].predict(X_test_reduced)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "    \n",
    "    # Append the metrics to the metrics list\n",
    "    metrics.append([model['name'], accuracy, precision, recall, f1])\n",
    "\n",
    "# Convert the metrics list into a DataFrame\n",
    "metrics_df = pd.DataFrame(metrics, columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "\n",
    "# Print the DataFrame\n",
    "metrics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search and Recommendation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "data = pd.read_csv('yelp_academic_dataset_review.csv')\n",
    "\n",
    "def search(keyword):\n",
    "    results = data[data['business_name'].str.contains(keyword, case=False)]\n",
    "    return results\n",
    "\n",
    "def recommend(user_id):\n",
    "    user_data = data[data['user_id'] == user_id]\n",
    "    user_preferences = user_data['category'].unique()\n",
    "\n",
    "    similar_businesses = data[data['category'].isin(user_preferences)]\n",
    "\n",
    "    recommended_businesses = similar_businesses.groupby('business_name').mean().sort_values(by='rating', ascending=False)\n",
    "    \n",
    "    return recommended_businesses\n",
    "\n",
    "#print(\"Search Results:\")\n",
    "#print(search_results[['business_name', 'category', 'rating']])\n",
    "\n",
    "user_id = '111'\n",
    "recommendations = recommend(user_id)\n",
    "#print(\"Recommendations for User\", user_id)\n",
    "#print(recommendations[['category', 'rating']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
