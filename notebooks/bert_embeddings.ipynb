{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c040105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "281185c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1242adfe8645d687303cde08f2220b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66af65db5a7b40a5915795b927a88f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68db669c6c94309b5810c5075273f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ff2adb8d304d1ea0b15b4c0a37d674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5f46ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_text_preparation(text, tokenizer):\n",
    "    \"\"\"\n",
    "    Preprocesses text input in a way that BERT can interpret.\n",
    "    \"\"\"\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1]*len(indexed_tokens)\n",
    "    # convert inputs to tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensor = torch.tensor([segments_ids])\n",
    "    return tokenized_text, tokens_tensor, segments_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cde2683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(tokens_tensor, segments_tensor, model):\n",
    "    \"\"\"\n",
    "    Obtains BERT embeddings for tokens.\n",
    "    \"\"\"\n",
    "    # gradient calculation id disabled\n",
    "    with torch.no_grad():\n",
    "      # obtain hidden states\n",
    "        outputs = model(tokens_tensor, segments_tensor)\n",
    "        hidden_states = outputs[2]\n",
    "    # concatenate the tensors for all layers\n",
    "    # use \"stack\" to create new dimension in tensor\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    # remove dimension 1, the \"batches\"\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    # swap dimensions 0 and 1 so we can loop over tokens\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    # intialized list to store embeddings\n",
    "    token_vecs_sum = []\n",
    "    # \"token_embeddings\" is a [Y x 12 x 768] tensor\n",
    "    # where Y is the number of tokens in the sentence\n",
    "    # loop over tokens in sentence\n",
    "    for token in token_embeddings:\n",
    "    # \"token\" is a [12 x 768] tensor\n",
    "    # sum the vectors from the last four layers\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "    return token_vecs_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8da3cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"bank\",\n",
    "         \"he eventually sold the shares back to the bank at a premium.\",\n",
    "         \"the bank strongly resisted cutting interest rates.\",\n",
    "         \"the bank will supply and buy back foreign currency.\",\n",
    "         \"the bank is pressing us for repayment of the loan.\",\n",
    "         \"the bank left its lending rates unchanged.\",\n",
    "         \"the river flowed over the bank.\",\n",
    "         \"tall, luxuriant plants grew along the river bank.\",\n",
    "         \"his soldiers were arrayed along the river bank.\",\n",
    "         \"wild flowers adorned the river bank.\",\n",
    "         \"two fox cubs romped playfully on the river bank.\",\n",
    "         \"the jewels were kept in a bank vault.\",\n",
    "         \"you can stow your jewellery away in the bank.\",\n",
    "         \"most of the money was in storage in bank vaults.\",\n",
    "         \"the diamonds are shut away in a bank vault somewhere.\",\n",
    "         \"thieves broke into the bank vault.\",\n",
    "         \"can I bank on your support?\",\n",
    "         \"you can bank on him to hand you a reasonable bill for your services.\",\n",
    "         \"don't bank on your friends to help you out of trouble.\",\n",
    "         \"you can bank on me when you need money.\",\n",
    "         \"i bank on your help.\"\n",
    "         ]\n",
    "from collections import OrderedDict\n",
    "context_embeddings = []\n",
    "context_tokens = []\n",
    "for sentence in sentences:\n",
    "    tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(sentence, tokenizer)\n",
    "    list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
    "    # make ordered dictionary to keep track of the position of each   word\n",
    "    tokens = OrderedDict()\n",
    "    # loop over tokens in sensitive sentence\n",
    "    for token in tokenized_text[1:-1]:\n",
    "        # keep track of position of word and whether it occurs multiple times\n",
    "        if token in tokens:\n",
    "            tokens[token] += 1\n",
    "        else:\n",
    "            tokens[token] = 1\n",
    "        # compute the position of the current token\n",
    "        token_indices = [i for i, t in enumerate(tokenized_text) if t == token]\n",
    "        current_index = token_indices[tokens[token]-1]\n",
    "        # get the corresponding embedding\n",
    "        token_vec = list_token_embeddings[current_index]\n",
    "\n",
    "        # save values\n",
    "        context_tokens.append(token)\n",
    "        context_embeddings.append(token_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcb6c38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "print(len(context_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cd8739",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
